{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configuración Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "# Rutas de los archivos\n",
    "ruta = '/gdrive/MyDrive/TEG-EEG/dataset_bonn/'\n",
    "ruta_F = ruta + 'F/'\n",
    "ruta_N = ruta + 'N/'\n",
    "ruta_O = ruta + 'O/'\n",
    "ruta_S = ruta + 'S/'\n",
    "ruta_Z = ruta + 'Z/'\n",
    "\n",
    "# Categorías de los subsets\n",
    "LABEL_ZO = 0 # Sujetos normales (sets Z y O). 200 en total\n",
    "LABEL_NF = 1 # Sujetos anormales señales epilépticas interictales (N y F). 200 en total\n",
    "LABEL_S = 2  # Sujetos anormales señales epilépticas ictales (S). 100 en total\n",
    "\n",
    "def leer_datos():\n",
    "    datos_0 = []\n",
    "    datos_1 = []\n",
    "    datos_2 = []\n",
    "    N = 0        # Cantidad de archivos\n",
    "\n",
    "    # Leer subset F\n",
    "    print('Leyendo subset F... (categoría 1')\n",
    "    for filename in os.listdir(ruta_F):\n",
    "        dato = np.loadtxt(ruta_F + filename)\n",
    "        dato = dato[:-1] # Tomar los primeros 4.096 datos, no incluir el dato 4.097\n",
    "        datos_1.append([dato, np.array([LABEL_NF])])\n",
    "        N += 1\n",
    "\n",
    "    # Leer subset N\n",
    "    print('Leyendo subset N... (categoría 1')\n",
    "    for filename in os.listdir(ruta_N):\n",
    "        dato = np.loadtxt(ruta_N + filename)\n",
    "        dato = dato[:-1] # Tomar los primeros 4.096 datos, no incluir el dato 4.097\n",
    "        datos_1.append([dato, np.array([LABEL_NF])])\n",
    "        N += 1\n",
    "\n",
    "    # Leer subset Z\n",
    "    print('Leyendo subset Z... (categoría 0')\n",
    "    for filename in os.listdir(ruta_Z):\n",
    "        dato = np.loadtxt(ruta_Z + filename)\n",
    "        dato = dato[:-1] # Tomar los primeros 4.096 datos, no incluir el dato 4.097\n",
    "        datos_0.append([dato, np.array([LABEL_ZO])])\n",
    "        N += 1\n",
    "\n",
    "    # Leer subset O\n",
    "    print('Leyendo subset O... (categoría 0')\n",
    "    for filename in os.listdir(ruta_O):\n",
    "        dato = np.loadtxt(ruta_O + filename)\n",
    "        dato = dato[:-1] # Tomar los primeros 4.096 datos, no incluir el dato 4.097\n",
    "        datos_0.append([dato, np.array([LABEL_ZO])])\n",
    "        N += 1\n",
    "\n",
    "    # Leer subset S\n",
    "    print('Leyendo subset S...(categoría 2)')\n",
    "    for filename in os.listdir(ruta_S):\n",
    "        dato = np.loadtxt(ruta_S + filename)\n",
    "        dato = dato[:-1] # Tomar los primeros 4.096 datos, no incluir el dato 4.097\n",
    "        datos_2.append([dato, np.array([LABEL_S])])\n",
    "        N += 1\n",
    "\n",
    "    print(f'Lectura terminada. {N} registros en total')\n",
    "\n",
    "    return datos_0, datos_1, datos_2\n",
    "\n",
    "datos_0, datos_1, datos_2 = leer_datos()\n",
    "\n",
    "\n",
    "# Convertir datos_0, datos_1 y datos_2 de listas\n",
    "# a arreglos de NumPy\n",
    "\n",
    "# Los arreglos resultantes tendran N (datos) x 4.097 columnas.\n",
    "# Las primeras 4.096 columnas son la señal EEG, la última columna es la\n",
    "# categoría\n",
    "\n",
    "def list_to_array(lista):\n",
    "    N = len(lista)\n",
    "    nx = 4096\n",
    "    ny = 1\n",
    "\n",
    "    array = np.zeros((N,nx+ny),dtype='float32')\n",
    "    for i, item in enumerate(lista):\n",
    "        x = item[0] # Señal EEG\n",
    "        y = item[1] # Categoría\n",
    "        xy = np.concatenate((x,y), axis=0)\n",
    "        array[i] = xy\n",
    "\n",
    "    return array\n",
    "\n",
    "datos_0_arr = list_to_array(datos_0)\n",
    "datos_1_arr = list_to_array(datos_1)\n",
    "datos_2_arr = list_to_array(datos_2)\n",
    "\n",
    "print(datos_0_arr.shape)\n",
    "print(datos_1_arr.shape)\n",
    "print(datos_2_arr.shape)\n",
    "\n",
    "\n",
    "# Extraer subsets de entrenamiento, validación y prueba\n",
    "def extraer_subsets(datos, pct_trn=0.8, pct_val=0.1, pct_tst=0.1, seed=23):\n",
    "    # Definir número de datos a extraer por cada subset\n",
    "    N = datos.shape[0]  # Cantidad total de datos\n",
    "    Ntrn = int(pct_trn*N) # Cantidad de datos de entrenamiento\n",
    "    Nval = int(pct_val*N) # Cantidad de datos de validación\n",
    "    Ntst = N-Ntrn-Nval    # Cantidad de datos de prueba\n",
    "\n",
    "    # Mezclar aleatoriamente las filas del set de datos\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(datos)\n",
    "\n",
    "    # Seleccionar los subsets\n",
    "    datos_trn = datos[0:Ntrn]\n",
    "    datos_val = datos[Ntrn:Ntrn+Nval]\n",
    "    datos_tst = datos[Ntrn+Nval:]\n",
    "\n",
    "    return datos_trn, datos_val, datos_tst\n",
    "\n",
    "# Ejecutar la función\n",
    "train_0, val_0, test_0 = extraer_subsets(datos_0_arr)\n",
    "train_1, val_1, test_1 = extraer_subsets(datos_1_arr)\n",
    "train_2, val_2, test_2 = extraer_subsets(datos_2_arr)\n",
    "\n",
    "print(train_0.shape)\n",
    "print(val_0.shape)\n",
    "print(test_0.shape)\n",
    "print(train_1.shape)\n",
    "print(val_1.shape)\n",
    "print(test_1.shape)\n",
    "print(train_2.shape)\n",
    "print(val_2.shape)\n",
    "print(test_2.shape)\n",
    "\n",
    "\n",
    "# Concatenar los arreglos train_0, 1 y 2 en train y lo mismo para validación\n",
    "# y entrenamiento\n",
    "\n",
    "train = np.concatenate((train_0,train_1,train_2))\n",
    "val = np.concatenate((val_0, val_1, val_2))\n",
    "test = np.concatenate((test_0, test_1, test_2))\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# Conformar los sets x_train/y_train, x_val/y_val, x_test/y_test\n",
    "\n",
    "def sets_xy(datos, seed=23):\n",
    "    # Mezclar datos aleatoriamente\n",
    "    # no hace falta mezclar\n",
    "    #np.random.seed(seed)\n",
    "    #np.random.shuffle(datos)\n",
    "    # get all columns except the last one\n",
    "    x = datos[:,0:-1]\n",
    "    y = datos[:,-1]\n",
    "\n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = sets_xy(train)\n",
    "x_val, y_val = sets_xy(val)\n",
    "x_test, y_test = sets_xy(test)\n",
    "\n",
    "# Estandarización\n",
    "\n",
    "def estandarizar(X, mu=None, sigma=None):\n",
    "\n",
    "    if mu and sigma:\n",
    "        X_s = (X-mu)/sigma\n",
    "    else:\n",
    "        # Calcular media y desviación\n",
    "        mu = np.mean(X)\n",
    "        sigma = np.std(X)\n",
    "\n",
    "        # Estandarizar\n",
    "        X_s = (X - mu)/sigma\n",
    "\n",
    "    return X_s, mu, sigma\n",
    "\n",
    "# Estandarizar los sets\n",
    "x_train_s, mu, sigma = estandarizar(x_train)\n",
    "x_val_s, _, _ = estandarizar(x_val, mu, sigma)\n",
    "x_test_s, _, _ = estandarizar(x_test, mu, sigma)\n",
    "\n",
    "print(x_test_s.mean())\n",
    "print(x_test_s.std())\n",
    "\n",
    "# Ventaneo\n",
    "def ventanear(X,Y):\n",
    "    X_v = []\n",
    "    Y_v = []\n",
    "\n",
    "    wsize = 128\n",
    "    #nwind = 4096/wsize\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        # Ventanear el registro \"x\"\n",
    "        # en teoria como son 400 registros por 16 ventaneos da un total 6400\n",
    "        x_v = np.reshape(x,(32,wsize))      # 16x256\n",
    "        y_v = np.repeat(y,32).reshape(32,1) # 16x1\n",
    "\n",
    "        X_v.append(x_v)\n",
    "        Y_v.append(y_v)\n",
    "\n",
    "    # Y convertir las listas X_v, Y_v a arreglos NumPy\n",
    "    X_v = np.vstack(X_v)\n",
    "    Y_v = np.vstack(Y_v)\n",
    "\n",
    "    np.random.shuffle(X_v)\n",
    "    np.random.shuffle(Y_v)\n",
    "\n",
    "    # flatten lo convierte en 1 dimension\n",
    "\n",
    "    return X_v, Y_v.flatten()\n",
    "\n",
    "\n",
    "print(x_train_s.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train_v, y_train_v = ventanear(x_train_s,y_train)\n",
    "x_val_v, y_val_v = ventanear(x_val_s,y_val)\n",
    "x_test_v, y_test_v = ventanear(x_test_s,y_test)\n",
    "\n",
    "print(x_train_v.shape)\n",
    "print(x_val_v.shape)\n",
    "print(x_test_v.shape)\n",
    "print(y_train_v.shape)\n",
    "print(y_val_v.shape)\n",
    "print(y_test_v.shape)\n",
    "\n",
    "\n",
    "# Además se convertirán Y_v_train, Y_v_test al formato one-hot\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_v_oh = to_categorical(y_train_v,3)\n",
    "y_val_v_oh = to_categorical(y_val_v,3)\n",
    "y_test_v_oh = to_categorical(y_test_v,3)\n",
    "\n",
    "print(y_train_v_oh.shape)\n",
    "print(y_val_v_oh.shape)\n",
    "print(y_test_v_oh.shape)\n",
    "\n",
    "# La función np.random.normal permite añadir el ruido Gaussiano\n",
    "# con las características deseadas a cada segmento de la señal\n",
    "\n",
    "# Se debe añadir ruido con el mismo promedio de la señal original\n",
    "# (loc=0.0) y con una desviación del 10% (0.1) de la señal\n",
    "# original (scale=0.1)\n",
    "noise_train = np.random.normal(loc=0.0, scale=0.1, size=x_train_v.shape)\n",
    "noise_val = np.random.normal(loc=0.0, scale=0.1, size=x_val_v.shape)\n",
    "# al de prueba investigar si se a;ade ruido\n",
    "noise_test = np.random.normal(loc=0.0, scale=0.1, size=x_test_v.shape)\n",
    "\n",
    "x_train_n = x_train_v + noise_train\n",
    "x_val_n = x_val_v + noise_val\n",
    "#x_test_n = x_test_v + noise_test\n",
    "x_test_n = x_test_v\n",
    "\n",
    "print(x_train_n.shape)\n",
    "#print(x_train_n[2])\n",
    "\n",
    "x_train_debug = x_train_n[:10]\n",
    "print(x_train_debug.shape)\n",
    "\n",
    "\n",
    "# 30-07-23 with all hyperparameters ok\n",
    "import tensorflow as tf\n",
    "\n",
    "func1 = 'relu'\n",
    "func2 = 'linear'\n",
    "\n",
    "tf.random.set_seed(157) # Reproducibilidad del entrenamiento\n",
    "\n",
    "# Definimos la entrada\n",
    "entrada = tf.keras.layers.Input(shape=(128,))\n",
    "\n",
    "# Agregamos una capa de convolución con 32 filtros, tamaño de kernel de 3, y activación relu\n",
    "entrada_convolucional = tf.keras.layers.Reshape((128, 1))(entrada)\n",
    "conv1 = tf.keras.layers.Conv1D(16, 3, padding='same', activation=func1)(entrada_convolucional)\n",
    "\n",
    "# Agregamos una capa de pooling con tamaño de pool de 2\n",
    "maxpool1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "maxpool1 = tf.keras.layers.Dropout(0.05)(maxpool1)\n",
    "\n",
    "encoder = tf.keras.layers.Dense(128, activation = func1,\n",
    "                                name='bottleneck')(maxpool1)\n",
    "encoder = tf.keras.layers.ActivityRegularization(l1=0.004)(encoder)\n",
    "\n",
    "# Aplicamos dropout para intentar reducir el overfitting\n",
    "encoder = tf.keras.layers.Dropout(0.05)(encoder)\n",
    "\n",
    "\n",
    "# Agregamos una capa de Reshape para aplanar la salida y pasarla a la capa conv1d_transpose_52\n",
    "transpuesta1 = tf.keras.layers.Conv1DTranspose(16, 3, padding='same', activation=func1)(encoder)\n",
    "transpuesta1 = tf.keras.layers.UpSampling1D(2)(transpuesta1)\n",
    "\n",
    "decoder_output = tf.keras.layers.Conv1DTranspose(1, 3, padding='same', activation=func2)(transpuesta1)\n",
    "\n",
    "# Aplicamos dropout para intentar reducir el overfitting\n",
    "decoder_output = tf.keras.layers.Dropout(0.05)(decoder_output)\n",
    "decoder_output = tf.keras.layers.Reshape((128,))(decoder_output)\n",
    "decoder_output = tf.keras.layers.Dense(128, activation=func2)(decoder_output)\n",
    "\n",
    "# Definimos el autoencoder como el modelo que toma la entrada y devuelve la salida del decoder\n",
    "autoencoder = tf.keras.models.Model(inputs=entrada, outputs=decoder_output)\n",
    "autoencoder.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "autoencoder.compile(\n",
    "    loss='mse',\n",
    "    optimizer=optimizer\n",
    "    )\n",
    "\n",
    "historial = autoencoder.fit(\n",
    "    x = x_train_n,\n",
    "    y = x_train_n,\n",
    "    batch_size=128,\n",
    "    epochs = 30,\n",
    "    validation_data = (x_val_n,x_val_n),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historial.history['loss'], label='Train')\n",
    "plt.plot(historial.history['val_loss'], label='Validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the bottleneck layer index\n",
    "bottleneck_index = [index for index, layer in enumerate(autoencoder.layers) if layer.name == 'bottleneck'][0]\n",
    "\n",
    "# Create the encoder model\n",
    "encoder_model = tf.keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[bottleneck_index].output)\n",
    "\n",
    "# Define the classification model\n",
    "classifier_input = tf.keras.Input(shape=encoder_model.output_shape[1:])\n",
    "x = classifier_input\n",
    "\n",
    "# If the encoder output is 3D, flatten it for the Dense layer\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(3, activation='softmax')(x)  # Assuming 3 classes\n",
    "classifier_model = tf.keras.Model(inputs=classifier_input, outputs=x)\n",
    "\n",
    "# Compile the classifier model\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier_model.summary()\n",
    "\n",
    "# Prepare the training data (encode the features using the encoder model)\n",
    "x_train_encoded = encoder_model.predict(x_train_n)\n",
    "x_test_encoded = encoder_model.predict(x_val_n)\n",
    "\n",
    "# Train the classifier\n",
    "classifier_history = classifier_model.fit(x_train_encoded, y_train_v_oh, \n",
    "                                          epochs=50,batch_size=128,\n",
    "                                          validation_data=(x_test_encoded, y_val_v_oh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LSTM layer:\n",
    "\n",
    "# Define the classification model\n",
    "classifier_input = tf.keras.Input(shape=encoder_model.output_shape[1:])\n",
    "x = classifier_input\n",
    "\n",
    "# LSTM Layer\n",
    "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# Flatten the output for the Dense layer (if necessary)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = tf.keras.layers.Dense(3, activation='softmax')(x)  # Assuming 3 classes\n",
    "classifier_model = tf.keras.Model(inputs=classifier_input, outputs=x)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the classifier model\n",
    "classifier_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier_model.summary()\n",
    "\n",
    "# Prepare the training data (encode the features using the encoder model)\n",
    "x_train_encoded = encoder_model.predict(x_train_n)\n",
    "x_test_encoded = encoder_model.predict(x_val_n)\n",
    "\n",
    "# Train the classifier\n",
    "classifier_history = classifier_model.fit(x_train_encoded, y_train_v_oh, \n",
    "                                          epochs=50,batch_size=128,\n",
    "                                          validation_data=(x_test_encoded, y_val_v_oh))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
