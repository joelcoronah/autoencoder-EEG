{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Google Drive setup\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "tf.random.set_seed(89)\n",
    "# File paths\n",
    "ruta = '/gdrive/MyDrive/TEG-EEG/dataset_bonn/'\n",
    "ruta_F = ruta + 'F/'\n",
    "ruta_N = ruta + 'N/'\n",
    "ruta_O = ruta + 'O/'\n",
    "ruta_S = ruta + 'S/'\n",
    "ruta_Z = ruta + 'Z/'\n",
    "\n",
    "# Subset categories\n",
    "LABEL_ZO = 0  # Normal subjects (sets Z and O), 200 in total\n",
    "LABEL_NF = 1  # Abnormal subjects interictal epileptic signals (N and F), 200 in total\n",
    "LABEL_S = 2   # Abnormal subjects ictal epileptic signals (S), 100 in total\n",
    "\n",
    "def read_data():\n",
    "    data_0 = []\n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    N = 0  # Number of files\n",
    "\n",
    "    # Read subset F\n",
    "    print('Reading subset F... (category 1)')\n",
    "    for filename in os.listdir(ruta_F):\n",
    "        datum = np.loadtxt(ruta_F + filename)\n",
    "        datum = datum[:-1]  # Take the first 4,096 data points, exclude the 4,097th data point\n",
    "        data_1.append([datum, np.array([LABEL_NF])])\n",
    "        N += 1\n",
    "\n",
    "    # Read subset N\n",
    "    print('Reading subset N... (category 1)')\n",
    "    for filename in os.listdir(ruta_N):\n",
    "        datum = np.loadtxt(ruta_N + filename)\n",
    "        datum = datum[:-1]  # Take the first 4,096 data points, exclude the 4,097th data point\n",
    "        data_1.append([datum, np.array([LABEL_NF])])\n",
    "        N += 1\n",
    "\n",
    "    # Read subset Z\n",
    "    print('Reading subset Z... (category 0)')\n",
    "    for filename in os.listdir(ruta_Z):\n",
    "        datum = np.loadtxt(ruta_Z + filename)\n",
    "        datum = datum[:-1]  # Take the first 4,096 data points, exclude the 4,097th data point\n",
    "        data_0.append([datum, np.array([LABEL_ZO])])\n",
    "        N += 1\n",
    "\n",
    "    # Read subset O\n",
    "    print('Reading subset O... (category 0)')\n",
    "    for filename in os.listdir(ruta_O):\n",
    "        datum = np.loadtxt(ruta_O + filename)\n",
    "        datum = datum[:-1]  # Take the first 4,096 data points, exclude the 4,097th data point\n",
    "        data_0.append([datum, np.array([LABEL_ZO])])\n",
    "        N += 1\n",
    "\n",
    "    # Read subset S\n",
    "    print('Reading subset S... (category 2)')\n",
    "    for filename in os.listdir(ruta_S):\n",
    "        datum = np.loadtxt(ruta_S + filename)\n",
    "        datum = datum[:-1]  # Take the first 4,096 data points, exclude the 4,097th data point\n",
    "        data_2.append([datum, np.array([LABEL_S])])\n",
    "        N += 1\n",
    "\n",
    "    print(f'Reading completed. Total {N} records')\n",
    "\n",
    "    return data_0, data_1, data_2\n",
    "\n",
    "def list_to_array(lst):\n",
    "    N = len(lst)\n",
    "    nx = 4096\n",
    "    ny = 1\n",
    "\n",
    "    array = np.zeros((N, nx+ny), dtype='float32')\n",
    "    for i, item in enumerate(lst):\n",
    "        x, y = item\n",
    "        array[i, :4096] = x\n",
    "        array[i, 4096:] = y\n",
    "\n",
    "    return array\n",
    "\n",
    "def partition_data(train_proportion, val_proportion, test_proportion, data_0, data_1, data_2):\n",
    "    array_0 = list_to_array(data_0)\n",
    "    array_1 = list_to_array(data_1)\n",
    "    array_2 = list_to_array(data_2)\n",
    "\n",
    "    np.random.shuffle(array_0)\n",
    "    np.random.shuffle(array_1)\n",
    "    np.random.shuffle(array_2)\n",
    "\n",
    "    N_0 = array_0.shape[0]\n",
    "    N_1 = array_1.shape[0]\n",
    "    N_2 = array_2.shape[0]\n",
    "\n",
    "    train_data = np.concatenate((array_0[:int(train_proportion*N_0)],\n",
    "                                 array_1[:int(train_proportion*N_1)],\n",
    "                                 array_2[:int(train_proportion*N_2)]), axis=0)\n",
    "\n",
    "    val_data = np.concatenate((array_0[int(train_proportion*N_0):int((train_proportion+val_proportion)*N_0)],\n",
    "                               array_1[int(train_proportion*N_1):int((train_proportion+val_proportion)*N_1)],\n",
    "                               array_2[int(train_proportion*N_2):int((train_proportion+val_proportion)*N_2)]), axis=0)\n",
    "\n",
    "    test_data = np.concatenate((array_0[int((train_proportion+val_proportion)*N_0):],\n",
    "                                array_1[int((train_proportion+val_proportion)*N_1):],\n",
    "                                array_2[int((train_proportion+val_proportion)*N_2):]), axis=0)\n",
    "\n",
    "    # Shuffle the data again for good measure\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(val_data)\n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_autoencoder_model(input_dim, encoding_dim):\n",
    "    # Encoder model\n",
    "    # inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    # encoder_layer1 = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "    # encoder_layer2 = tf.keras.layers.Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    # encoder = tf.keras.Model(inputs, encoder_layer2, name='encoder')\n",
    "\n",
    "    # # Decoder model\n",
    "    # decoder_layer1 = tf.keras.layers.Dense(256, activation='relu')(encoder_layer2)\n",
    "    # decoder_output = tf.keras.layers.Dense(input_dim, activation='sigmoid')(decoder_layer1)\n",
    "    # decoder = tf.keras.Model(encoder_layer2, decoder_output, name='decoder')\n",
    "\n",
    "    # # Autoencoder model\n",
    "    # autoencoder_output = decoder(encoder(inputs))\n",
    "    # autoencoder_model = tf.keras.Model(inputs, autoencoder_output, name='autoencoder')\n",
    "    # autoencoder_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # return autoencoder_model, encoder\n",
    "\n",
    "\n",
    "    ## Encoder stage 1\n",
    "    # convolutional layer 1\n",
    "    print(input_dim)\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    print(inputs)\n",
    "\n",
    "\n",
    "    encoder_layer1 = tf.keras.layers.Reshape((input_dim, 1))(inputs)\n",
    "    encoder_layer1 = tf.keras.layers.Conv1D(16, 10, activation='relu', padding='same')(encoder_layer1)\n",
    "\n",
    "    # bach normalization layer 1\n",
    "    encoder_layer1 = tf.keras.layers.BatchNormalization()(encoder_layer1)\n",
    "\n",
    "    # Activation layer 1\n",
    "    encoder_layer1 = tf.keras.layers.Activation('relu')(encoder_layer1)\n",
    "\n",
    "    # Pooling Layer 1 \n",
    "    encoder_layer1 = tf.keras.layers.MaxPooling1D(2, padding='same')(encoder_layer1)\n",
    "\n",
    "    ## Encoder stage 2\n",
    "    # convolutional layer 2\n",
    "    encoder_layer2 = tf.keras.layers.Conv1D(8, 10, activation='relu', padding='same')(encoder_layer1)\n",
    "\n",
    "    # bach normalization layer 2\n",
    "    encoder_layer2 = tf.keras.layers.BatchNormalization()(encoder_layer2)\n",
    "\n",
    "    # Activation layer 2\n",
    "    encoder_layer2 = tf.keras.layers.Activation('relu')(encoder_layer2)\n",
    "\n",
    "    # Pooling Layer 2\n",
    "    encoder_layer2 = tf.keras.layers.MaxPooling1D(2, padding='same')(encoder_layer2)\n",
    "\n",
    "\n",
    "    ## Encoder stage 3\n",
    "    # convolutional layer 3\n",
    "    encoder_layer3 = tf.keras.layers.Conv1D(8, 10, activation='relu', padding='same')(encoder_layer2)\n",
    "\n",
    "    # bach normalization layer 3\n",
    "    encoder_layer3 = tf.keras.layers.BatchNormalization()(encoder_layer3)\n",
    "\n",
    "    # Activation layer 3\n",
    "    encoder_layer3 = tf.keras.layers.Activation('relu')(encoder_layer3)\n",
    "\n",
    "    # Pooling Layer 3\n",
    "    encoder_layer3 = tf.keras.layers.MaxPooling1D(2, padding='same')(encoder_layer3)\n",
    "\n",
    "    encoder = tf.keras.layers.Dense(128, activation = 'relu',\n",
    "                                name='bottleneck')(encoder_layer3)\n",
    "\n",
    "    encoder = tf.keras.layers.ActivityRegularization(l1=0.004)(encoder)\n",
    "\n",
    "    encoder = tf.keras.layers.Dropout(0.05)(encoder)\n",
    "\n",
    "    encoder = tf.keras.Model(inputs, encoder_layer3, name='encoder')\n",
    "\n",
    "\n",
    "    ## Decoder layer 1\n",
    "    # Convolutional layer 1\n",
    "\n",
    "    decoder_layer1 = tf.keras.layers.Conv1D(8, 10, activation='relu', padding='same')(encoder_layer3)\n",
    "\n",
    "    # barch normalization layer 1\n",
    "    decoder_layer1 = tf.keras.layers.BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    # Activation layer 1\n",
    "\n",
    "    decoder_layer1 = tf.keras.layers.Activation('relu')(decoder_layer1)\n",
    "\n",
    "    # Pooling Layer 1\n",
    "    decoder_layer1 = tf.keras.layers.UpSampling1D(2)(decoder_layer1)\n",
    "\n",
    "    ## Decoder layer 2\n",
    "    # Upsampling layer 2\n",
    "\n",
    "    decoder_layer2 = tf.keras.layers.UpSampling1D(2)(decoder_layer1)\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    decoder_layer2 = tf.keras.layers.Conv1D(8, 10, activation='relu', padding='same')(decoder_layer2)\n",
    "\n",
    "    # bach normalization layer 2\n",
    "    decoder_layer2 = tf.keras.layers.BatchNormalization()(decoder_layer2)\n",
    "\n",
    "    # Activation layer 2\n",
    "    decoder_layer2 = tf.keras.layers.Activation('relu')(decoder_layer2)\n",
    "\n",
    "    ## Decoder layer 3 \n",
    "    # Upsampling layer 3\n",
    "\n",
    "    decoder_layer3 = tf.keras.layers.UpSampling1D(2)(decoder_layer2)\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    decoder_layer3 = tf.keras.layers.Conv1D(16, 10, activation='relu', padding='same')(decoder_layer3)\n",
    "\n",
    "    # bach normalization layer 3\n",
    "    decoder_layer3 = tf.keras.layers.BatchNormalization()(decoder_layer3)\n",
    "\n",
    "    # Activation layer 3\n",
    "    decoder_layer3 = tf.keras.layers.Activation('relu')(decoder_layer3)\n",
    "\n",
    "   # reconstruction layer\n",
    "    decoder_layer3 = tf.keras.layers.Conv1D(1, 10, activation='sigmoid', padding='same')(decoder_layer3)\n",
    "\n",
    "    # Reshape layer\n",
    "    decoder_layer3 = tf.keras.layers.Reshape((input_dim,))(decoder_layer3)\n",
    "\n",
    "    # dense layer\n",
    "    # decoder_layer3 = tf.keras.layers.Dense(input_dim, activation='sigmoid')(decoder_layer3)\n",
    "\n",
    "\n",
    "    decoder_layer3 = tf.keras.layers.Dropout(0.05)(decoder_layer3)\n",
    "\n",
    "    decoder_layer3 = tf.keras.layers.Dense(128, activation = 'relu',\n",
    "                                name='bottleneck')(decoder_layer3)\n",
    "\n",
    "\n",
    "\n",
    "    decoder = tf.keras.Model(encoder_layer3, decoder_layer3, name='decoder')\n",
    "\n",
    "    ## Autoencoder model\n",
    "    autoencoder_output = decoder(encoder(inputs))\n",
    "    autoencoder_model = tf.keras.Model(inputs, autoencoder_output, name='autoencoder')\n",
    "    autoencoder_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return autoencoder_model, encoder\n",
    "\n",
    "# Ventaneo\n",
    "def ventanear(X,Y):\n",
    "    X_v = []\n",
    "    Y_v = []\n",
    "\n",
    "    wsize = 128\n",
    "    #nwind = 4096/wsize\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        # Ventanear el registro \"x\"\n",
    "        # en teoria como son 400 registros por 16 ventaneos da un total 6400\n",
    "        x_v = np.reshape(x,(32,wsize))      # 16x256\n",
    "        y_v = np.repeat(y,32).reshape(32,1) # 16x1\n",
    "\n",
    "        X_v.append(x_v)\n",
    "        Y_v.append(y_v)\n",
    "\n",
    "    # Y convertir las listas X_v, Y_v a arreglos NumPy\n",
    "    X_v = np.vstack(X_v)\n",
    "    Y_v = np.vstack(Y_v)\n",
    "\n",
    "    np.random.shuffle(X_v)\n",
    "    np.random.shuffle(Y_v)\n",
    "\n",
    "    # flatten lo convierte en 1 dimension\n",
    "\n",
    "    return X_v, Y_v.flatten()\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder_model, x_train, x_val, epochs, batch_size):\n",
    "    history = autoencoder_model.fit(x_train, x_train,\n",
    "                                    validation_data=(x_val, x_val),\n",
    "                                    epochs=epochs,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    verbose=1)\n",
    "    return history\n",
    "\n",
    "def create_encoder_model(input_shape, encoder):\n",
    "    inputs = tf.keras.Input(shape=input_shape[1:])\n",
    "    outputs = encoder(inputs)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"encoder_model\")\n",
    "    return model\n",
    "\n",
    "def create_classifier_model(encoder, num_classes):\n",
    "    inputs = tf.keras.Input(shape=(encoder.output.shape[1],))\n",
    "    classifier_layer1 = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "    classifier_output = tf.keras.layers.Dense(num_classes, activation='softmax')(classifier_layer1)\n",
    "    model = tf.keras.Model(inputs, classifier_output, name='classifier')\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_classifier(classifier_model, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "    history = classifier_model.fit(x_train, y_train,\n",
    "                                   validation_data=(x_val, y_val),\n",
    "                                   epochs=epochs,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   verbose=1)\n",
    "    return history\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred):\n",
    "    # Calculate true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return f1_score\n",
    "\n",
    "def calculate_sensitivity_specificity(y_true, y_pred):\n",
    "    # Calculate true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "    return sensitivity, specificity\n",
    "\n",
    "def normalization(X, mu=None, sigma=None):\n",
    "  # data normalization\n",
    "\n",
    "    if mu and sigma:\n",
    "        X_s = (X-mu)/sigma\n",
    "    else:\n",
    "        mu = np.mean(X)\n",
    "        sigma = np.std(X)\n",
    "        X_s = (X - mu)/sigma\n",
    "\n",
    "    return X_s, mu, sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Google Drive setup\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "\n",
    "    # File paths\n",
    "    ruta = '/gdrive/MyDrive/dataset_bonn/'\n",
    "    ruta_F = ruta + 'F/'\n",
    "    ruta_N = ruta + 'N/'\n",
    "    ruta_O = ruta + 'O/'\n",
    "    ruta_S = ruta + 'S/'\n",
    "    ruta_Z = ruta + 'Z/'\n",
    "\n",
    "    # Read data from file paths\n",
    "    data_0, data_1, data_2 = read_data()\n",
    "\n",
    "    # Partition data into train, validation, and test sets\n",
    "    train_proportion = 0.8\n",
    "    val_proportion = 0.1\n",
    "    test_proportion = 0.1\n",
    "    train_data, val_data, test_data = partition_data(train_proportion, val_proportion, test_proportion, data_0, data_1, data_2)\n",
    "\n",
    "    # Convert data into the required format (x_data and y_data)\n",
    "    x_train = train_data[:, :-1]\n",
    "    y_train = train_data[:, -1].astype(int)\n",
    "    x_val = val_data[:, :-1]\n",
    "    y_val = val_data[:, -1].astype(int)\n",
    "    x_test = test_data[:, :-1]\n",
    "    y_test = test_data[:, -1].astype(int)\n",
    "\n",
    "    # Normalize data \n",
    "    x_train, mu, sigma = normalization(x_train)\n",
    "    x_val, _, _ = normalization(x_val, mu, sigma)\n",
    "    x_test, _, _ = normalization(x_test, mu, sigma)\n",
    "\n",
    "    # Ventaneo\n",
    "    x_train, y_train = ventanear(x_train,y_train)\n",
    "    x_val, y_val = ventanear(x_val,y_val)\n",
    "    x_test, y_test = ventanear(x_test,y_test)\n",
    "\n",
    "    # Define the input and encoding dimensions for the autoencoder\n",
    "    input_dim = x_train.shape[1]\n",
    "    encoding_dim = 32  # You can adjust this dimension as needed\n",
    "\n",
    "    # Create and train the Autoencoder model\n",
    "    autoencoder_model, encoder = create_autoencoder_model(128, encoding_dim)\n",
    "    autoencoder_model.summary()\n",
    "    train_autoencoder(autoencoder_model, x_train, x_val, epochs=100, batch_size=128)\n",
    "\n",
    "    # # Encode the data using the trained autoencoder\n",
    "    # x_train_encoded = encoder.predict(x_train)\n",
    "    # x_val_encoded = encoder.predict(x_val)\n",
    "    # x_test_encoded = encoder.predict(x_test)\n",
    "\n",
    "    # # Define the number of classes for the classifier\n",
    "    # num_classes = len(np.unique(y_train))\n",
    "\n",
    "    # # Create one-hot encoded labels for the classifier\n",
    "    # y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    # y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "    # y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # # Create and train the Classifier model\n",
    "    # classifier_model = create_classifier_model(encoder, num_classes)\n",
    "    # classifier_model.summary()\n",
    "    # train_classifier(classifier_model, x_train_encoded, y_train_onehot, x_val_encoded, y_val_onehot, epochs=100, batch_size=64)\n",
    "\n",
    "    # # Evaluate the models on test data and calculate metrics\n",
    "    # y_pred = classifier_model.predict(x_test_encoded)\n",
    "    # y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    # y_test_labels = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "    # f1score = calculate_f1_score(y_test_labels, y_pred_labels)\n",
    "    # sensitivity, specificity = calculate_sensitivity_specificity(y_test_labels, y_pred_labels)\n",
    "\n",
    "\n",
    "    # num_thresholds = 100\n",
    "    # thresholds = np.linspace(0, 1, num_thresholds)\n",
    "    # tpr_values = []\n",
    "    # fpr_values = []\n",
    "\n",
    "    # for threshold in thresholds:\n",
    "    #      y_pred_thresholded = (y_pred[:, 1] >= threshold).astype(int)\n",
    "    #      tpr = np.sum((y_test_labels == 1) & (y_pred_thresholded == 1)) / np.sum(y_test_labels == 1)\n",
    "    #      fpr = np.sum((y_test_labels == 0) & (y_pred_thresholded == 1)) / np.sum(y_test_labels == 0)\n",
    "    #      tpr_values.append(tpr)\n",
    "    #      fpr_values.append(fpr)\n",
    "\n",
    "    #       # Plot the ROC curve\n",
    "    # plt.figure()\n",
    "    # plt.plot(fpr_values, tpr_values, color='darkorange', lw=2, label='ROC curve')\n",
    "    # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Receiver Operating Characteristic')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n",
    "    # # Display the calculated metrics in the command prompt\n",
    "    # print(\"Sensitivity: {:.4f}\".format(sensitivity))\n",
    "    # print(\"Specificity: {:.4f}\".format(specificity))\n",
    "    # print(\"F1 Score: {:.4f}\".format(f1score))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
